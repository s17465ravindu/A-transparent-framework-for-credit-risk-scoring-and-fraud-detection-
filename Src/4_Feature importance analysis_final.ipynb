{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/qpxKR+c3yglBOFDpZjRG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict"],"metadata":{"id":"3PLS8qccrypP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dn3gy0xjr4MA","executionInfo":{"status":"ok","timestamp":1733299265660,"user_tz":-330,"elapsed":22074,"user":{"displayName":"Ravindu Rathnaweera","userId":"17042501109574684741"}},"outputId":"f59682dc-bcb4-4e1d-e7e0-24b6fbe0199f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/ISC Case study/Data/pre_processed_dataset_balanced.csv')"],"metadata":{"id":"O3a57DTysP5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = data.drop(columns=['charge_off_status'])\n","y = data['charge_off_status']\n"],"metadata":{"id":"iXdv_DL3sSBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/ISC Case study/Data/logit_model.pkl'\n","\n","with open(model_path, 'rb') as model_file:\n","    logit_model = pickle.load(model_file)\n","\n","# Verify the model type\n","type(logit_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"DoJuk4xjr-Y6","executionInfo":{"status":"ok","timestamp":1733299299808,"user_tz":-330,"elapsed":1066,"user":{"displayName":"Ravindu Rathnaweera","userId":"17042501109574684741"}},"outputId":"2381c45d-a9e1-4dc3-87da-90bc489093e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sklearn.linear_model._logistic.LogisticRegression"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>sklearn.linear_model._logistic.LogisticRegression</b><br/>def __init__(penalty=&#x27;l2&#x27;, *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=&#x27;lbfgs&#x27;, max_iter=100, multi_class=&#x27;deprecated&#x27;, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py</a>Logistic Regression (aka logit, MaxEnt) classifier.\n","\n","In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n","scheme if the &#x27;multi_class&#x27; option is set to &#x27;ovr&#x27;, and uses the\n","cross-entropy loss if the &#x27;multi_class&#x27; option is set to &#x27;multinomial&#x27;.\n","(Currently the &#x27;multinomial&#x27; option is supported only by the &#x27;lbfgs&#x27;,\n","&#x27;sag&#x27;, &#x27;saga&#x27; and &#x27;newton-cg&#x27; solvers.)\n","\n","This class implements regularized logistic regression using the\n","&#x27;liblinear&#x27; library, &#x27;newton-cg&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27; and &#x27;lbfgs&#x27; solvers. **Note\n","that regularization is applied by default**. It can handle both dense\n","and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n","floats for optimal performance; any other input format will be converted\n","(and copied).\n","\n","The &#x27;newton-cg&#x27;, &#x27;sag&#x27;, and &#x27;lbfgs&#x27; solvers support only L2 regularization\n","with primal formulation, or no regularization. The &#x27;liblinear&#x27; solver\n","supports both L1 and L2 regularization, with a dual formulation only for\n","the L2 penalty. The Elastic-Net regularization is only supported by the\n","&#x27;saga&#x27; solver.\n","\n","Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.\n","\n","Parameters\n","----------\n","penalty : {&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;, None}, default=&#x27;l2&#x27;\n","    Specify the norm of the penalty:\n","\n","    - `None`: no penalty is added;\n","    - `&#x27;l2&#x27;`: add a L2 penalty term and it is the default choice;\n","    - `&#x27;l1&#x27;`: add a L1 penalty term;\n","    - `&#x27;elasticnet&#x27;`: both L1 and L2 penalty terms are added.\n","\n","    .. warning::\n","       Some penalties may not work with some solvers. See the parameter\n","       `solver` below, to know the compatibility between the penalty and\n","       solver.\n","\n","    .. versionadded:: 0.19\n","       l1 penalty with SAGA solver (allowing &#x27;multinomial&#x27; + L1)\n","\n","dual : bool, default=False\n","    Dual (constrained) or primal (regularized, see also\n","    :ref:`this equation &lt;regularized-logistic-loss&gt;`) formulation. Dual formulation\n","    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n","    n_samples &gt; n_features.\n","\n","tol : float, default=1e-4\n","    Tolerance for stopping criteria.\n","\n","C : float, default=1.0\n","    Inverse of regularization strength; must be a positive float.\n","    Like in support vector machines, smaller values specify stronger\n","    regularization.\n","\n","fit_intercept : bool, default=True\n","    Specifies if a constant (a.k.a. bias or intercept) should be\n","    added to the decision function.\n","\n","intercept_scaling : float, default=1\n","    Useful only when the solver &#x27;liblinear&#x27; is used\n","    and self.fit_intercept is set to True. In this case, x becomes\n","    [x, self.intercept_scaling],\n","    i.e. a &quot;synthetic&quot; feature with constant value equal to\n","    intercept_scaling is appended to the instance vector.\n","    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n","\n","    Note! the synthetic feature weight is subject to l1/l2 regularization\n","    as all other features.\n","    To lessen the effect of regularization on synthetic feature weight\n","    (and therefore on the intercept) intercept_scaling has to be increased.\n","\n","class_weight : dict or &#x27;balanced&#x27;, default=None\n","    Weights associated with classes in the form ``{class_label: weight}``.\n","    If not given, all classes are supposed to have weight one.\n","\n","    The &quot;balanced&quot; mode uses the values of y to automatically adjust\n","    weights inversely proportional to class frequencies in the input data\n","    as ``n_samples / (n_classes * np.bincount(y))``.\n","\n","    Note that these weights will be multiplied with sample_weight (passed\n","    through the fit method) if sample_weight is specified.\n","\n","    .. versionadded:: 0.17\n","       *class_weight=&#x27;balanced&#x27;*\n","\n","random_state : int, RandomState instance, default=None\n","    Used when ``solver`` == &#x27;sag&#x27;, &#x27;saga&#x27; or &#x27;liblinear&#x27; to shuffle the\n","    data. See :term:`Glossary &lt;random_state&gt;` for details.\n","\n","solver : {&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;, &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;},             default=&#x27;lbfgs&#x27;\n","\n","    Algorithm to use in the optimization problem. Default is &#x27;lbfgs&#x27;.\n","    To choose a solver, you might want to consider the following aspects:\n","\n","    - For small datasets, &#x27;liblinear&#x27; is a good choice, whereas &#x27;sag&#x27;\n","      and &#x27;saga&#x27; are faster for large ones;\n","    - For multiclass problems, only &#x27;newton-cg&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27; and\n","      &#x27;lbfgs&#x27; handle multinomial loss;\n","    - &#x27;liblinear&#x27; and &#x27;newton-cholesky&#x27; can only handle binary classification\n","      by default. To apply a one-versus-rest scheme for the multiclass setting\n","      one can wrapt it with the `OneVsRestClassifier`.\n","    - &#x27;newton-cholesky&#x27; is a good choice for `n_samples` &gt;&gt; `n_features`,\n","      especially with one-hot encoded categorical features with rare\n","      categories. Be aware that the memory usage of this solver has a quadratic\n","      dependency on `n_features` because it explicitly computes the Hessian\n","      matrix.\n","\n","    .. warning::\n","       The choice of the algorithm depends on the penalty chosen and on\n","       (multinomial) multiclass support:\n","\n","       ================= ============================== ======================\n","       solver            penalty                        multinomial multiclass\n","       ================= ============================== ======================\n","       &#x27;lbfgs&#x27;           &#x27;l2&#x27;, None                     yes\n","       &#x27;liblinear&#x27;       &#x27;l1&#x27;, &#x27;l2&#x27;                     no\n","       &#x27;newton-cg&#x27;       &#x27;l2&#x27;, None                     yes\n","       &#x27;newton-cholesky&#x27; &#x27;l2&#x27;, None                     no\n","       &#x27;sag&#x27;             &#x27;l2&#x27;, None                     yes\n","       &#x27;saga&#x27;            &#x27;elasticnet&#x27;, &#x27;l1&#x27;, &#x27;l2&#x27;, None yes\n","       ================= ============================== ======================\n","\n","    .. note::\n","       &#x27;sag&#x27; and &#x27;saga&#x27; fast convergence is only guaranteed on features\n","       with approximately the same scale. You can preprocess the data with\n","       a scaler from :mod:`sklearn.preprocessing`.\n","\n","    .. seealso::\n","       Refer to the User Guide for more information regarding\n","       :class:`LogisticRegression` and more specifically the\n","       :ref:`Table &lt;Logistic_regression&gt;`\n","       summarizing solver/penalty supports.\n","\n","    .. versionadded:: 0.17\n","       Stochastic Average Gradient descent solver.\n","    .. versionadded:: 0.19\n","       SAGA solver.\n","    .. versionchanged:: 0.22\n","        The default solver changed from &#x27;liblinear&#x27; to &#x27;lbfgs&#x27; in 0.22.\n","    .. versionadded:: 1.2\n","       newton-cholesky solver.\n","\n","max_iter : int, default=100\n","    Maximum number of iterations taken for the solvers to converge.\n","\n","multi_class : {&#x27;auto&#x27;, &#x27;ovr&#x27;, &#x27;multinomial&#x27;}, default=&#x27;auto&#x27;\n","    If the option chosen is &#x27;ovr&#x27;, then a binary problem is fit for each\n","    label. For &#x27;multinomial&#x27; the loss minimised is the multinomial loss fit\n","    across the entire probability distribution, *even when the data is\n","    binary*. &#x27;multinomial&#x27; is unavailable when solver=&#x27;liblinear&#x27;.\n","    &#x27;auto&#x27; selects &#x27;ovr&#x27; if the data is binary, or if solver=&#x27;liblinear&#x27;,\n","    and otherwise selects &#x27;multinomial&#x27;.\n","\n","    .. versionadded:: 0.18\n","       Stochastic Average Gradient descent solver for &#x27;multinomial&#x27; case.\n","    .. versionchanged:: 0.22\n","        Default changed from &#x27;ovr&#x27; to &#x27;auto&#x27; in 0.22.\n","    .. deprecated:: 1.5\n","       ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n","       From then on, the recommended &#x27;multinomial&#x27; will always be used for\n","       `n_classes &gt;= 3`.\n","       Solvers that do not support &#x27;multinomial&#x27; will raise an error.\n","       Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n","       still want to use OvR.\n","\n","verbose : int, default=0\n","    For the liblinear and lbfgs solvers set verbose to any positive\n","    number for verbosity.\n","\n","warm_start : bool, default=False\n","    When set to True, reuse the solution of the previous call to fit as\n","    initialization, otherwise, just erase the previous solution.\n","    Useless for liblinear solver. See :term:`the Glossary &lt;warm_start&gt;`.\n","\n","    .. versionadded:: 0.17\n","       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n","\n","n_jobs : int, default=None\n","    Number of CPU cores used when parallelizing over classes if\n","    multi_class=&#x27;ovr&#x27;&quot;. This parameter is ignored when the ``solver`` is\n","    set to &#x27;liblinear&#x27; regardless of whether &#x27;multi_class&#x27; is specified or\n","    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n","    context. ``-1`` means using all processors.\n","    See :term:`Glossary &lt;n_jobs&gt;` for more details.\n","\n","l1_ratio : float, default=None\n","    The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only\n","    used if ``penalty=&#x27;elasticnet&#x27;``. Setting ``l1_ratio=0`` is equivalent\n","    to using ``penalty=&#x27;l2&#x27;``, while setting ``l1_ratio=1`` is equivalent\n","    to using ``penalty=&#x27;l1&#x27;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a\n","    combination of L1 and L2.\n","\n","Attributes\n","----------\n","\n","classes_ : ndarray of shape (n_classes, )\n","    A list of class labels known to the classifier.\n","\n","coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n","    Coefficient of the features in the decision function.\n","\n","    `coef_` is of shape (1, n_features) when the given problem is binary.\n","    In particular, when `multi_class=&#x27;multinomial&#x27;`, `coef_` corresponds\n","    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n","\n","intercept_ : ndarray of shape (1,) or (n_classes,)\n","    Intercept (a.k.a. bias) added to the decision function.\n","\n","    If `fit_intercept` is set to False, the intercept is set to zero.\n","    `intercept_` is of shape (1,) when the given problem is binary.\n","    In particular, when `multi_class=&#x27;multinomial&#x27;`, `intercept_`\n","    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n","    outcome 0 (False).\n","\n","n_features_in_ : int\n","    Number of features seen during :term:`fit`.\n","\n","    .. versionadded:: 0.24\n","\n","feature_names_in_ : ndarray of shape (`n_features_in_`,)\n","    Names of features seen during :term:`fit`. Defined only when `X`\n","    has feature names that are all strings.\n","\n","    .. versionadded:: 1.0\n","\n","n_iter_ : ndarray of shape (n_classes,) or (1, )\n","    Actual number of iterations for all classes. If binary or multinomial,\n","    it returns only 1 element. For liblinear solver, only the maximum\n","    number of iteration across all classes is given.\n","\n","    .. versionchanged:: 0.20\n","\n","        In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed\n","        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n","\n","See Also\n","--------\n","SGDClassifier : Incrementally trained logistic regression (when given\n","    the parameter ``loss=&quot;log_loss&quot;``).\n","LogisticRegressionCV : Logistic regression with built-in cross validation.\n","\n","Notes\n","-----\n","The underlying C implementation uses a random number generator to\n","select features when fitting the model. It is thus not uncommon,\n","to have slightly different results for the same input data. If\n","that happens, try with a smaller tol parameter.\n","\n","Predict output may not match that of standalone liblinear in certain\n","cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;`\n","in the narrative documentation.\n","\n","References\n","----------\n","\n","L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n","    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n","    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n","\n","LIBLINEAR -- A Library for Large Linear Classification\n","    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n","\n","SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n","    Minimizing Finite Sums with the Stochastic Average Gradient\n","    https://hal.inria.fr/hal-00860051/document\n","\n","SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).\n","        :arxiv:`&quot;SAGA: A Fast Incremental Gradient Method With Support\n","        for Non-Strongly Convex Composite Objectives&quot; &lt;1407.0202&gt;`\n","\n","Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n","    methods for logistic regression and maximum entropy models.\n","    Machine Learning 85(1-2):41-75.\n","    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n","\n","Examples\n","--------\n","&gt;&gt;&gt; from sklearn.datasets import load_iris\n","&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n","&gt;&gt;&gt; X, y = load_iris(return_X_y=True)\n","&gt;&gt;&gt; clf = LogisticRegression(random_state=0).fit(X, y)\n","&gt;&gt;&gt; clf.predict(X[:2, :])\n","array([0, 0])\n","&gt;&gt;&gt; clf.predict_proba(X[:2, :])\n","array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n","       [9.7...e-01, 2.8...e-02, ...e-08]])\n","&gt;&gt;&gt; clf.score(X, y)\n","0.97...</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 810);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["\n","\n","# Get the feature names and corresponding coefficients\n","coefficients = logit_model.coef_.flatten()  # Assuming 'model' is your trained logistic regression model\n","features = X.columns  # Assuming X is your feature matrix (DataFrame)\n","\n","# Create a DataFrame to store the feature names and their coefficients\n","feature_importance_df = pd.DataFrame({\n","    'feature': features,\n","    'coefficient': coefficients,\n","    'abs_coefficient': np.abs(coefficients)\n","})\n","\n","# Sort the features by absolute coefficient value (higher magnitude means more important)\n","sorted_features = feature_importance_df.sort_values(by='abs_coefficient', ascending=False)\n","\n","# Display top 10 most important features\n","print(sorted_features.head(15))\n","\n","# You can set a threshold for importance and filter out less important features\n","threshold = 0.1  # Example threshold\n","important_features = sorted_features[sorted_features['abs_coefficient'] > threshold]['feature']\n","\n","# Now, filter your dataset with only the important features\n","X_filtered = X[important_features]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4sSyLRrrt5t","executionInfo":{"status":"ok","timestamp":1733300126731,"user_tz":-330,"elapsed":691,"user":{"displayName":"Ravindu Rathnaweera","userId":"17042501109574684741"}},"outputId":"f7ce0f3e-aa6d-4098-f6c2-b8828f4fec82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                    feature  coefficient  abs_coefficient\n","3                        delinquency_status     2.308163         2.308163\n","10             number_of_defaulted_accounts    -0.415980         0.415980\n","14  applications_submitted_during_odd_hours    -0.294963         0.294963\n","11        new_accounts_opened_last_12months     0.261067         0.261067\n","12  multiple_applications_short_time_period    -0.243668         0.243668\n","6                 payment_methods_high_risk    -0.237827         0.237827\n","9             number_of_delinquent_accounts    -0.202794         0.202794\n","15                 watchlist_blacklist_flag    -0.185399         0.185399\n","16                      public_records_flag    -0.158300         0.158300\n","13               unusual_submission_pattern    -0.144421         0.144421\n","2                                fico_score    -0.134087         0.134087\n","20                       occupation_encoded    -0.122714         0.122714\n","4             number_of_credit_applications     0.095204         0.095204\n","1                              income_level     0.088577         0.088577\n","0                                       age    -0.072255         0.072255\n"]}]}]}